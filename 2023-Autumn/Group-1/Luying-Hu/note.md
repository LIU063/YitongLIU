### Week 9
# Learning to Schedule Tasks with Deadline and Throughput Constraints 
学习在截止时间和吞吐量约束下进行任务调度
## 摘要
我们考虑任务调度场景，其中控制器每次激活K种任务类型中的一种。每个任务引发**随机**完成时间，仅在任务完成后才能获得奖励。控制器对所有任务类型的完成时间和奖励分布的统计数据是**未知的**。控制器需要学习如何调度任务，以在**给定的时间段T内最大化累积奖励**。受实际场景启发，我们要求设计的策略满足系统**吞吐量约束**。此外，我们引入了**中断机制**，以终止超过某个截止时间的正在进行的任务。为解决这个调度问题，我们将其建模为**带有截止时间和吞吐量约束的在线学习问题**。然后，我们表征了最优的离线策略，并基于李亚普诺夫（Lyapunov）方法开发了高效的在线学习算法。我们证明我们的在线学习算法达到O(√T)的遗憾值（regret），并且没有违反约束（zero constraint violations）。我们还进行了模拟实验，以评估我们开发的学习算法的性能。  

## I. 介绍
### 场景
一个控制器处理具有**随机完成时间和未知奖励/效用**的任务，控制器安排这些任务以在给定的时间间隔内**最大化累积奖励**。具体来说，考虑一个基站（BS），它从K个异构的感知源中收集时间敏感的信息，以做出实时决策。每个源通过独特的上行通道与BS连接以进行数据传输。由于通道干扰，每次只能激活一个通道（即一个源）以传输数据包。假设BS需要在时间段T ≥ 0内安排传输轨迹以获取来自这些源的信息。在每个尝试中（n = 1, 2, 3, ...），BS激活一个属于K的源k以传输一个数据包。如果第n次尝试通过通道k传输，它需要**X<sub>k,n</sub>> 0**的时间段，并且成功传输后BS将获得奖励**R<sub>k,n</sub>≥ 0**（例如，已更新信息的价值）。
### 存在问题：
根据以前文献中的常见假设，每个源的奖励和传输时间都是独立且相同分布的，服务器的调度问题可以自然地建模为一个受预算约束的多臂老虎机问题。然而，这种受预算约束的老虎机模型未能捕捉到网络场景中的两个独特挑战：**数据新鲜度和吞吐量约束**。第一个挑战数据新鲜度出现在许多**时间敏感的应用**中，它们对信息新鲜度或接收的数据包的信息时代（AoI）有要求。因此，如果一次试验的数据传输超过了某个特定的时间阈值，数据就会过时，传输就毫无价值。第二个挑战吞吐量来自实践中的常见**服务质量（QoS）要求**。例如，BS希望为及时和高效的决策制定保证一定的聚合吞吐量。
### 解决方法： 
在论文中，研究了在**中断机制和吞吐量约束**下任务调度背景下的探索与利用的权衡。在多臂老虎机（MAB）设置中，每个臂对应于一个任务类型，每个任务需要随机完成时间，并在完成后获得随机奖励。控制器在给定的时间跨度内最大化累积奖励，但一开始不知道每个臂的奖励和完成时间的统计数据。除了上面讨论的受预算约束的多臂老虎机模型之外，引入了中断机制，即任务将在截止日期后被终止。中断机制导致零奖励。此外，受实际应用的启发，我们为控制器引入了吞吐量约束。吞吐量约束大大复杂化了探索与利用的权衡的设计。为了充分捕捉这些特性，我们提出了一个在线算法，该算法融合了来自**老虎机理论、李亚普诺夫优化和统计估计的新设计和分析技术**。
### 主要贡献：  
1、我们考虑了一个通用的任务调度问题，同时具有截止日期和吞吐量约束。据我们所知，我们是第一个将该问题建模为多臂老虎机问题的研究，其中：  （a）每个尝试（或决策）都包括随机的时间消耗和中断机制；  （b）受到严格的“预算”类型时间约束；  （c）具有（随机的）吞吐量约束，这是许多应用所要求的。  
2、当臂的统计数据已知时，我们使用续航理论确定一个可操作的随机策略，具有O(1)的最优性差距。（在算法和优化领域，O(1) optimality gap 是一个重要的性能度量，它表明算法或策略在实际应用中能够接近最佳解决方案，而不受问题规模的限制。）  
3、当臂的统计数据未知时，我们提出一种新的基于李亚普诺夫的方法来开发一种高效且性能显著的算法，即在时间间隔T内达到O(√T)的遗憾值和零约束违反。我们对该算法的分析基于续航理论、老虎机优化以及用于概率估计的新型集中不等式。（续航理论是概率论中的一个分支，用于研究随机事件发生的时间间隔。它通常用于分析和建模周期性事件或随机事件之间的时间间隔。）
### A.动机场景
**云计算中的任务分配**：形式上，有K个异构服务器（或者我们可以称它们为工作节点以避免混淆），可以依次租用给每个任务，也就是说，在任何时刻只能租用一个工作节点。如果选择工作节点k∈[K]来执行第n个任务，那么任务将持续一个随机时间X<sub>k,n</sub>，并且如果成功完成，将生成一个随机奖励R<sub>k,n</sub>。如果X<sub>k,n</sub> ≤ d，其中d是任务的截止日期，那么任务将成功完成。否则，任务将失败，对控制器不生成奖励，但会浪费时间直到截止日期。控制器的目标是在给定的时间跨度内最大化总体效用，同时满足系统的吞吐量要求，即在单位时间内完成的任务数量。

**无线视频流传输**：考虑用户从远程服务器请求视频流传输。视频流传输由多个数据块组成，每个数据块有K个可用比特率版本供下载（不同比特率版本具有不同的块大小，并对用户产生不同的效用）。用户按顺序下载这些数据块，一个接一个。在这里，数据块的下载受时间约束，这是由于视频流传输协议或有限的连接时间（例如，在车辆自组织网络中用户在高速移动），即当下载时间超过阈值d时，数据块的下载会终止。如果某个数据块下载失败，用户会尝试重新下载，直到成功为止。需要注意的是，数据块的低加载速率可能导致用户的体验质量不佳，如应用播放失败。因此，用户的目标是在给定时间内最大化其总体获得的效用，同时满足最低成功数据块下载速率的要求。
### B.相关工作
**受预算约束的老虎机问题**：由于在我们的模型中，总的（随机）时间消耗必须满足严格的预算约束T，因此我们的问题可以被视为一种受预算约束的老虎机问题。在经典的受预算约束的老虎机问题中，已有大量研究，其目标是在随机环境下在背包约束下最大化累积奖励。这个基本模型已经扩展到线性背景设置，组合半老虎机设置，对抗性设置等。我们的模型与这些研究有很大的不同之处，**因为它们没有将中断或取消机制和吞吐量约束纳入学习问题中**。

**学习任务调度**：我们的在线学习问题在不同的背景下已经得到研究，例如，服务器分配，作业分派，以及无线流调度等。在这些工作中，关注作业分派中未知服务速率（即未知完成时间）的队列长度最小化；旨在开发在截止日期约束的流调度中最终避免任何中断而不知道通道统计信息的有效调度策略。最相关的工作是，他们也从在线学习的角度将中断机制引入了服务器分配问题中。**然而，他们考虑了全信息反馈模型，其中每次试验中所有臂的奖励和完成时间都向控制器揭示。此外，他们只关注根据每个臂的累积奖励最大化总效用，因此不涉及QoS考虑**。

**学习辅助的李亚普诺夫优化**：近年来，基于李亚普诺夫的方法已广泛用于在线学习问题，目标是在背包（资源）约束、公平性约束、能量约束、信息时代（AoI）约束或切换成本约束等条件下最大化总奖励。最近的工作还利用了基于李亚普诺夫的方法来解决线性背景老虎机问题中的约束在线学习问题。**我们的工作不同于这些研究，因为由于中断和每次试验的随机时间消耗的叠加以及QoS考虑，由臂产生的奖励可能会被“拒绝”。**

## II. 问题表述
我们研究了在给定时间跨度T > 0内的顺序决策过程，每次试验（或决策）都有K个臂（或任务类型）可供选择。如果在第n次试验中选择臂k，那么它需要完成时间X<sub>k,n</sub> > 0，成功完成后，控制器可以获得奖励R<sub>k,n</sub>∈[0, R<sub>max</sub>]。对于每个k，(X<sub>k,n</sub>, R<sub>k,n</sub>)是独立分布且在n上同分布（iid），但它们的统计数据对控制器来说是未知的。需要注意的是，在我们的模型中，奖励R<sub>k,n</sub>和完成时间X<sub>k,n</sub>可能存在相关性。我们还允许完成时间X<sub>k,n</sub>可能无界甚至是重尾分布。不失一般性，我们假设对于所有k，1 ≤ E[X<sub>k,1</sub>] < +∞。在我们的框架中，由于时间和截止日期要求，如果在截止日期d（d ≥ mink E[X<sub>k,n</sub>]）之前没有完成，则控制器将中断正在进行的试验，并且不会获得奖励。顺序决策过程将继续进行，直到给定的时间跨度T超出为止。因此，臂的完成时间与其获得的奖励一样重要。
为了数学描述这个过程，我们将Γ<sup>π</sup><sub>n</sub>表示在策略π下第n次试验选择的臂，这个策略只基于过去的观察结果，没有未来信息。在我们的模型中，我们考虑了老虎机反馈设置，也就是说，控制器只能在第n次试验后观察到向量：  
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/2792744b-e1d7-4824-ac37-421ee8512b5b"></div>
给定时间跨度T，根据策略π启动试验的数量是：
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/142f99cc-6a96-4cd2-a45b-5fd26b0e67b6"></div>
策略π下的总累积奖励为：
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/932d20c8-c2a5-4bf5-b39c-02c2eac834c0"></div>
需要注意的是，设计旨在最大化累积奖励的策略可能会导致低吞吐量（单位时间内成功完成的试验数量）。为了解决这一服务质量（QoS）问题，我们为控制器引入了以下吞吐量约束：
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/37369bf7-6f51-4a5f-9eaf-026efbda0cd3"></div>
因此，控制器的目标是找到一个在线策略π<sup>OPT</sup>（最佳策略），满足：
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/ad4c290b-1871-4ee7-8d0f-eefed3f250f5"></div>
然而，由于{X<sub>k,n</sub>, R<sub>k,n</sub>}序列及其统计数据是事先未知的，因此找到π<sup>OPT</sup>是不可能的，我们的目标是设计一个在线学习策略π，该策略在与π<sup>OPT</sup>相比具有良好的竞争性能。对于这一目标，性能度量指标是遗憾值和约束违反，定义如下：
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/fc0d7a88-39a4-4242-93df-f978f48c3fe0"></div>

这些公式用于定量评估策略 π 的性能，其中 "Regret" 表示了策略与最佳策略之间的性能差距，而 "Constraint Violation" 表示了策略是否满足了约束条件。研究人员通常希望设计策略，以最小化 "Regret" 并确保 "Constraint Violation" 接近零，以获得最佳的性能和可行性。
其中，OPT(T)是策略π<sup>OPT</sup>的期望累积奖励。需要注意的是，最大化累积奖励等同于最小化遗憾值。我们的目标是获得一个次线性的遗憾值和随着给定时间跨度T迅速减小的约束违反。**由于臂的统计数据是未知的**，为了实现这一目标，我们必须在老虎机类型反馈中**平衡利用和探索**之间的权衡。在我们的问题中，由于随机约束和中断机制，实现低遗憾的这种权衡变得**复杂**。后面我们将设计一个高效的**在线策略来优化这个权衡**(权衡探索未知选项和利用已知选项的决策过程)以进行最佳学习。

## III. 最优离线策略的近似
需要注意的是，即使在离线设置中，对于我们的问题，计算最优策略也具有非常高的计算复杂性，这使得它对于在线学习来说是难以处理的。为了获得学习算法设计和遗憾分析的可行基准，在本节中，我们研究了当**所有统计信息都已知**时具有明显良好性能的**近似算法**。首先，我们展示存在一个具有**有界遗憾的离线随机策略**。对于任意p = (p1, ..., pK) ∈ ∆K，我们将π(p)表示为随机策略，它以概率pk选择臂k进行所有试验。根据**续航理论**的结果，在随机策略π(p)下，单位时间的时间平均奖励limT→∞Rπ(p)(T)/T和时间平均吞吐量limT→∞E[PNπ(p)(T)n=1I{XIπ(p)n,n ≤ d}/T]分别收敛到正常数r(p)和c(p)，它们的定义如下：
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/79f4cf44-6a1c-42b2-b6bc-672373811c9a"></div>

r(p)表示在策略π(p)下的**单位时间内的平均奖励**，c(p)表示在策略π(p)下的**单位时间内的平均吞吐量**。这些参数是与策略p相关的，它们是评估策略性能的关键指标。接下来，我们将介绍一个离线稳态随机策略，该策略具有有界遗憾，可用于性能评估和学习算法的设计。直观上，奖励率较高的随机策略可以在时间预算T用完之前获得更多的奖励，而完成率较高的随机策略可以保证更大的吞吐量。然而，由于奖励较高的手臂也可能具有较长的完成时间，因此任何随机策略增加其奖励率都可能会降低其完成率。因此，当根据随机策略选择臂时，**控制器应该平衡其奖励率和完成率之间的权衡**，以在保证吞吐量约束的同时最大化获得的奖励。

**Lagrange Multiplier（拉格朗日乘子）**：在约束优化问题中，拉格朗日乘子是用来考虑约束条件的方法之一。它们通过将约束条件引入目标函数，从而将**优化问题转化为一个无约束问题**。λn 是与约束 c(p) ≥ α 相关的拉格朗日乘子，它用于平衡最大化奖励和满足约束条件之间的权衡。  
**Virtual Queue（虚拟队列）**：虚拟队列 Qn 用于跟踪约束违反的 "债务"，即约束条件在前 n 次决策中的违反情况。它在每个决策点更新，考虑了任务的完成时间和约束条件的关系。通过虚拟队列，算法试图管理约束条件的满足情况。  
**Balance Parameter（平衡参数）**：1/V 是平衡参数，用于调整虚拟队列与拉格朗日乘子之间的关系。这个参数可以用来平衡约束条件的满足和最大化奖励之间的权衡。  
**Pessimistic Mechanism（悲观机制）**：为了确保零约束违反（constraint violation），算法引入了一种悲观机制，通过使虚拟队列过高估计约束违反来实现。这意味着虚拟队列会预测比实际情况更严格的约束条件，以确保满足约束。  

Theorem 1说明了在离线设置中，当T足够大时，π<sup>off</sup>可以保证O(√T)的遗憾（regret），并且零约束违反（zero constraint violation）。在在线学习设置中，π<sup>off</sup>将作为我们设计高效算法的指导。这个定理表明在足够大的时间范围内，π<sup>off</sup>的性能是很好的，但对于小时间范围可能需要更复杂的算法来实现相似的性能。
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/fe8eb5d5-cd78-4040-8b3d-b4fa3d4c8944"></div>

## IV. 基于lyapunov的在线策略
在这一部分，我们将介绍在线学习设置中的一种高效算法，它具有低遗憾和约束违反。由于算法在开始时并不了解（r<sub>k</sub>，c<sub>k</sub>）的信息，因此它使用（r<sub>k</sub>，c<sub>k</sub>）的经验估计值，同时引入一种上置信度修正，以鼓励探索未知信息。为了确保探索不会导致过多的遗憾和约束违反，我们需要计算（r<sub>k</sub>，c<sub>k</sub>）的经验均值估计的高概率置信半径。需要注意的是，r<sub>k</sub>和c<sub>k</sub>都是两个期望值之比，因此传统的赌博领域的集中不等式不适用于这种情况。因此，接下来，我们将开发适用于估计奖励率和约束率的新型集中半径。
<div align=center><img width="500" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/bc6c9dd4-6640-4f05-9670-fc229926e357"></div>

## V. 多个中断时间选择
在这一部分，我们考虑了控制器可以确定每个试验的中断时间的情景。具体来说，在每次选择一个任务后，控制器可以从有限的离散集合T = {t1, t2, ..., tL} 中选择一个中断时间，例如，云计算中的小时/天或信道调度中的时间槽。这里tL可以是+∞，也就是说，控制器认为等待任务完成是最优的。在这种情况下，需要注意，对于[K] × T中的任何一对（k，t），在确定性策略下，观察到的随机过程在n上是独立同分布的。  
定理3考虑 R<sub>k,n</sub> 独立于X<sub>k,n</sub>的情况。 (a) 如果X<sub>k,n</sub> ∼ Exp(λ)，则对于所有t>0，r<sub>k,t</sub>= E[R<sub>k,1</sub>]λ且c<sub>k,t</sub> = λ，即中断时间的选择不会影响奖励率和完成率。 (b) r<sub>k,t</sub>和r<sub>k,t</sub>是高斯分布、均匀分布、逻辑分布和伽玛完成时间分布的t单调递增函数。  
定理3的一些观察结果：(a) 当完成时间由于无记忆特性呈指数分布时，中断不会产生影响。 (b) 对于许多轻尾完成时间分布，最佳中断时间是无限的。  

## VII. 实验结果
在本节中，我们进行数值实验来验证我们开发的算法的理论保证。实验设置：我们使用伯努利分布式奖励和重尾分布式完成时间评估 K = 4 个臂的π<sup>off</sup>和π<sup>on</sup>算法。手臂统计设计如下：
<div align=center><img width="700" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/006124d5-f9f4-415e-bffd-e3d05ddc1814"></div>

我们可以观察到，臂1和臂2被设置为奖励率低但完成率高，臂3和臂4则相反。我们还设置deadline d=10和α=0.1来强制算法以一定的频率选择arms 1和arms 2以满足吞吐量要求，因为高奖励率的arms的完成率都低于0.1。因此，控制器应该在这些臂之间进行权衡，因为选择其中一个臂的任何静态策略都将导致线性遗憾或线性约束违反。在我们的模拟设置中，我们选择V=√T、δ=d/√T，图中的每个点都是 100 个独立实验的平均值。
结果和分析。图1绘制了π<sup>off</sup>、π<sup>on</sup>和最优随机策略π(p*)在不同时间间隔T下的奖励率R(T)/T。它表明离线和在线设计都达到了最优设计的速率，如我们的理论结果所示。图1(b)和图2(b) 证实了π<sup>on</sup>违反约束的缩放行为，即当 T 足够大时，其随速率 O(1/T) 变化，这也在我们的理论结果中得到了揭示。他们还表明，当 T 足够大时，如果选择适当的 V 和 δ 值，我们确实可以获得负约束违反。
<div align=center><img width="700" alt="image" src="https://github.com/UNIC-Lab/Weekly-Report/assets/121794362/abbd6f51-6a0d-4514-96b7-50f7e6db121e"></div>


### Week 8

1、完成了嵌入VNFs的代码，能跑通，但还有点问题。

2、学习处理时延的论文。

[1]Q. Liu and Z. Fang, "Learning to Schedule Tasks with Deadline and Throughput Constraints," IEEE INFOCOM 2023 - IEEE Conference on Computer Communications, New York City, NY, USA, 2023, pp. 1-10, doi: 10.1109/INFOCOM53939.2023.10228901.

### Week 7

1、项目工作。

### Week 6

1、学完了代码，把VNFs改成复用类型，参照论文中的算法嵌入VNFs。

[1]G. Wang, S. Zhou, S. Zhang, Z. Niu and X. Shen, "SFC-Based Service Provisioning for Reconfigurable Space-Air-Ground Integrated Networks," in IEEE Journal on Selected Areas in Communications, vol. 38, no. 7, pp. 1478-1489, July 2020, doi: 10.1109/JSAC.2020.2986851.

2、撰写查新报告。

### Week 4

1、学了A2C（Advantage Actor-Critic）算法和DGL；

2、学习贺博的论文。


### Week 3

1、撰写项目申请书；

2、学习贺博的代码。


### Week 2

1、配置环境；

2.学完后思考如何利用Graph Transformer。
- GAT的self-attention只计算邻居节点，而Transformer的self-attention会考虑所有的节点。
- Graph Transformer结合了Transformer的核心（全局关注）和GNN的核心（考虑图的拓扑属性）。
