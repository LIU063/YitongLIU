# 3月23日周报
## 学习内容：
 1.学习了一下KL散度和匈牙利算法，尝试对算法利用匈牙利算法进行两两配对，从而构造一种新型拓扑结构
 2.学习沈师兄写的代码和之前的相关工作。
 3.匹配好的客户端如何再利用联邦分割进行训练有了一定的思路。目前算法思路如下：
### 算法：好朋友sfl
服务器初始化W层模型的模型参数定义为 $W_0 $，客户端上传自身的状态信息（ $D_i$ ，数据集大小; $C_i$ :计算能力; $P_i$ 数据集数据分布,   $(X_i,Y_i)$    位置坐标)
服务器根据各客户端上传的状态信息，主要是数据分布之间的差异性以及计算能力,还有通信范围与位置远近，根据匈牙利算法算好匹配搭档。
匹配具体流程：
首先根据客户端其距离远近，计算能力，数据异质性的差异映射到权重矩阵里，然后利用匈牙利算法得到最佳匹配值。     
具体如下：位置差异：将客户端Ui位置分别定义成   $(X_i,Y_i)$   ,客户端之间的间隔距离是 $disti=(X_i-X_j)^2＋（Y_i-Y_j)^2$ 将通信范围定义100,有两种特殊情况，若间隔距离都大于100，或者算的自己与自己的位置距离，我们将这两种特殊情况下通信距离都定义成无穷。总的生成的位置差异矩阵设成 `dist_ matrix` 。

计算能力：不同客户端的计算能力定义成 $C_i$ ，显然那计算能力的差异可以用最小二乘法表示成         $(C_i-C_j)^2$        ，当是自己和自己的时候，这种情况不是我们想要考虑的，所以我们将其设为-1.总的计算能力矩阵我们将其定义成 $C$ 

数据异质性：根据客户端定义几种不同类型的分布，此处定义成均匀，高斯，对数，指数分布的数据，然后我们利用迪利克雷分布将其范围缩至 $(0,1)$ 之间，再利用Kl散度分别表示出不同分布之间的差异值，由于KL散度正常情况输出值也肯定是非负数，所以自己和自己计算时，我们将其KL散度设置成为-1.总的数据差异矩阵设为d.
数据集大小的差异：     `dataset_ size_ difference`   ，也用最小二乘法表示。

位置差异，计算能力，数据异质性我们解决之后，我们将其映射到一个w权重矩阵里，定义成             $w = `dist_matrix `/ (1/2 * c + 1/4* d+1/4*`dataset_size_difference`)$        

再根据匈牙利算法输入我们的成本矩阵W，然后得到最佳匹配的结果      $（U_i，V_j)$        。
同时服务器计算客户端的向前传播步长和总聚合权重（这里总聚合权重主要指配对后的两个客户端当作一个整体，每个整体之间的总数据集的大小显然不同，为了方便之后服务器聚合时加权进行，故需要定义。）组队后将其定义为       $M$      ，每两个组队后的客户端将其定义成一个      $M_i$       ,M中一共包含M个        $M_i$       ，即组队后的两个小客户端的数据集总数为      $D$       ，即每一个      $D_i$      里面是     $d_i$      的大小加上     $d_j$      的大小，由此算出组队后聚合权重是              $Ai=\frac{D_i}{\sum_{r=1}^m D_i}$              ，服务器计算其传播步骤                $L_i=\frac{C_i}{C_i+C_j}W$             ;(此处的i,j是匹配好的对应的结果）,内部聚合权重为                     $a_i=\frac{d_i}{d_i+d_j}$             .     
其中一个小客户端相较于所有客户端聚合权重其实应是       $A_j * a_i$       服务器发送（   $W_0$     ,匹配好的搭档     $V_j$  ,     $L_i$    ,      $a_i$    )给每个客户端      $U_i$        。
图片是目前设想的伪代码：
<img width="548" alt="屏幕截图 2023-03-22 195316" src="https://user-images.githubusercontent.com/122032188/226929365-e7da4f36-97b7-4084-9b17-cdc19c18d5f3.png">

 
里面具体内部向前传播过程
在环内正常情况会产生2个客户端，他们的位置距离是很近的，这个因素在传播过程中就不需要再考虑了。
在每个通信轮次中，匹配后的两个客户端和两个客户端之间是没有通信的，但是匹配后客户端和服务器之间是并联的，每个客户端的前向传播可以分成2个部分；我们将这两个客户端分别设置为A，B。假设一个神经网络的层数一共有N+1层，即A拥有的神经网络层数Fa←{L0，L1，...Ln},B拥有的神经网络层数Fb←{Ln+1,Ln+2,...LN}。
### 6G大会相关内容
对6G的概念有了一定的认知，同时对6G的相关创新有所了解，例如基于现今无线网络的新通信方法必然产生，包括THZ和RISs,AI通信，以及其适应性和可持续性方面的考虑；然后计算创新方面：主要是基于边缘创新方面，关于AI创新主要是知识驱动，至于AI那种是我们需要的？有几个方面：解释性（即所有模型必须执行可解释性的决定和预测）；有根据性（即所有模型都有能力去分析，理解，和重现）概括性（即所有模型有泛化能力，和可转移的学习能力）分割和合作性（这个应该就是联邦学习方面的研究），绿色可持续性，以及终身和可连续性（从而确保学习不会断层）
### 下周计划：
尝试将匹配好的客户端的前向传播训练代码解决。


```python
import numpy as np
import math
from scipy.spatial import distance
from scipy.stats import entropy
from scipy.stats import dirichlet
#算计算能力的差异
# 随机生成四个一维点坐标
points = np.random.rand(4, 1) * 200#4个客户端计算能力随机为0-200
c = np.zeros((len(points),len(points)))
for i in range(len(points)):
    for j in range(len(points)):
        if i != j:
            c[i][j] = math.pow(points[i] - points[j],2)
        else:
            c[i][j]= -1#由于计算能力采用的是最小二乘法，所以合理情况下，不能为负数，为负数的情况指的是自己和自己，相当于矩阵的对角线
# print(c)
        
#计算距离的差异性
# 随机生成四个二维点坐标
points1 = np.random.rand(4, 2) * 200

# 计算不同点之间的距离矩阵
dist_matrix = distance.cdist(points1, points1, 'euclidean')
#设置通信范围
communication_range = 100
for i in range(len(dist_matrix)):
    for j in range(len(dist_matrix[i])):
        if dist_matrix[i][j] > communication_range  or dist_matrix[i][j] == 0:#当客户端自己和自己时，距离设成无穷，以及大于通信范围时。
            dist_matrix[i][j] = 99999999
# print(dist_matrix)

#考虑数据差异性
d = np.zeros((len(points),len(points))) 
# 生成均匀分布的数据
uniform_data =  np.random.dirichlet(np.random.uniform(0, 1, 10))

# 生成高斯分布的数据
gaussian_data = (np.random.normal(0, 1, 10))

# 生成对数分布的数据
log_data = np.random.dirichlet(np.random.lognormal(0, 1, 10))

# 生成指数分布的数据
exponential_data = np.random.dirichlet(np.random.exponential(1, 10))
 

# 对高斯分布和指数分布数据进行截断
truncated_gaussian_data =  np.random.dirichlet(np.clip(gaussian_data, 0.1, 1))#阶段
truncated_exponential_data = np.clip(exponential_data, 0.1, 1)

# 对对数分布数据进行对数变换，可以将其转化成正态分布的数据
log_transformed_data = np.log(log_data)
#再将转化成正态分布的数据进行一个截断。
truncated_log_transformed_data = np.clip(log_transformed_data, 0.1, 1)

temp = []
temp.append(uniform_data)
temp.append(truncated_gaussian_data)
temp.append(truncated_log_transformed_data)
temp.append(truncated_exponential_data)
for i in range(len(temp)):
    for j in range(len(temp)):
        if i != j:
            d[i][j] = entropy(temp[i],temp[j])
        else:
            d[i][j] = -1
# print(c)
#计算数据集大小的差异
dataset_size=np.random.rand(4, 1) * 10000
dataset_size_difference= np.zeros((len(points),len(points)))
for i in range(len(dataset_size)):
    for j in range(len(dataset_size)):
        if i != j:
            dataset_size_difference[i][j] = math.pow(dataset_size[i] - dataset_size[j],2)
        else:
            dataset_size_difference[i][j]= -1
w = dist_matrix / (1/2 * c + 1/4 * d + 1/4 *dataset_size_difference )
# print(dataset_size_difference)
for i in range(len(w)):
    for j in range(len(w[i])):
        if i == j:
            w[i][j] = 19999
        if w[i][j] == math.inf:
            w[i][j] = 10000
# print(w)
#输出权重矩阵       

from scipy.optimize import linear_sum_assignment as linear_assignment# 求解最小权重完美匹配问题
assignments = linear_assignment(w)
print(assignments)
#利用匈牙利算法成功匹配

 
dataset_size_number = np.zeros((len(dataset_size)))
# print(len(assignments[0]) // 2)
for i in range(len(assignments[0])):
     dataset_size_number[i] = math.floor(dataset_size[assignments[0][i]]) + math.floor(dataset_size[assignments[1][i]])
# print(dataset_size)
# print(dataset_size_number)
all_dataset_size=sum(dataset_size_number)/2
A=np.zeros(len(dataset_size_number))
for i in range(len(A)):
    A[i]=dataset_size_number[i]/all_dataset_size
    
L = np.zeros((len(dataset_size)))
weight = np.random.rand(1) * 10000

for i in range(len(dataset_size)):
    L[i] = (points[assignments[0][i]] / (points[assignments[0][i]] + points[assignments[1][i]])) * weight[0]
print(L)

bingzaiyiqi =np.zeros((4,2))
for i in range(len(assignments[0])):
    bingzaiyiqi[i][0]=assignments[0][i]
    bingzaiyiqi[i][1]=assignments[1][i]
print(bingzaiyiqi)
```

    (array([0, 1, 2, 3], dtype=int64), array([1, 0, 3, 2], dtype=int64))
    [4850.46858271 4450.41405403 5491.50746918 3809.37516756]
    [[0. 1.]
     [1. 0.]
     [2. 3.]
     [3. 2.]]
    
# 3月29日周报
## 6G大会感想
### 3月23日感想
3月23日上午华为的卢建民讲了一下关于面向未来的绿色网络，其中绿色涉及方方面面，牵扯网络接口，资源分配等。当链路层设计达到天花板时需要在网络拓扑上进行一定程度的改造，尽量让收发端尽量靠近来抵御通信距离，有几种手段，一个是分布式，一个是以用户为中心的联邦学习（可以让多个节点来服务一个小区），还有就是增加一些中间节点，然后我们接着了解一下联邦学习相关情况：多样的边缘学习架构一共有以下几种：联邦学习，Decentralized learning ,模型分割学习，然后如何减少能源消耗？有两种方面：要不就是在AI上想办法，要不就是在AI和通信的结合上想办法，在AI上我们可以用稀疏化，量化，投影从而大幅度降低AI的开销。
例如在联邦学习里我们可以不把所有的参数往上传，可以采用一些方法对模型参数或者梯度进行缩减，从而降低开销。

### 3月24日感想，
3月24号上午有些论坛涉及联邦学习的一些部分，概述一下所学并提出一定感想。
在联邦学习的数字无线设计中：
1. 由于全局和本地联邦学习模型参数需要无线连接的连接交流交换，所以无线传输有故障将会影响Fl性能。
2. 基站(BS)必须接收来自设备的更新本地模型，因此传输延迟和能源消耗是一定要考虑的问题。
3. 无线网络核心要素影响学习模型：功率，宽带，接入
该论坛给出的仿真结果是当考虑FL训练参数时优化无线通信因素可以有效的改善训练时的损失，提高模型性能
但如果不考虑FL训练参数只优化无线通信因素将会导致更坏的情况。
增加RBs数量可以改善证明精度。
这将是以后我们设计联邦分割学习之后，根据这些核心要素可以引入进一步探讨，从而找到联合的最佳匹配情况。同时AI算法设计时要充分考虑网络特性，充分利用领域知识。并且在网络优化过程中可以带来AI性能的提升，是算力网络设计的必要部分。
浙江大学的张朝阳的论坛也有一些可以借鉴的东西，有一些相关的文献可以看一下
Asynchronous Federated Learning Architecture[TWC, 2022]
Hierarchical Federated Learning Architecture[GC’ 2022]
Joint Model Split and Neural Architecture Search [ICC' 2022]
Over-the-Air Split Learning Architecture [JSAC 2023]
Wireless Graph Neural Network Architecture [WCM, 2023]
异步联邦学习架构面临一些问题：（传统联邦学习不能很好的适应于无线蜂窝网络）
1.同样面临着很多异质性的难题即客户端能力的异构性和通道条件的异构性。
2.模型到达的数量过时且不确定
3.数据的异构性
解决方案：  1.服务器根据用户模型的新鲜度和重要度计算贡献矩阵，进行全局融合;
   2. 每个用户根据全局模型的新鲜度及其与训练不足模型的相关性来决定是否更新
以后采用聚合时可以除了考虑联邦平均那种聚合之外，可以在一定程度上考虑下这种聚合模式。
需求是缺乏一款适用于不同复杂无线场景的通用AI体系结构。
## 对上周的毕设工作进行了一定程度的修改
需要改变的几个点在于：
1.向前传播过程中的传播步长是只和匹配之后两者的计算能力有关，但是聚合过程那个系数之前是想着先进行一个小环反向传播，然后传给服务器，服务器再进行一次模型的加权聚合，但是这样比较繁琐，还是直接调整一下
            $ a_i$               比较好。                                      
2.之前是把客户端的计算能力，数据集大小，数据集分布，以及位置信息都随机分布，但正常情况且为了和模型做对比，最好的方式应该是读取各个客户端的状态信息进行统计，然后导入，其次为了仿真方便，我们更倾向于定义传播步长即Li，那传播步长与计算能力是存在一定的关系的，故打算在仿真过程中映射权重矩阵时，找到一个与Li相关的关系。（在此这样的话，最小二乘法衡量其差距就不太可取。）        
    代码还在创作阶段，下周争取将这个匹配后的结果与sfl结合起来。
