# 周报
## 学习内容：
 1.学习了一下KL散度和匈牙利算法，尝试对算法利用匈牙利算法进行两两配对，从而构造一种新型拓扑结构
 2.学习沈师兄写的代码和之前的相关工作。
 3.匹配好的客户端如何再利用联邦分割进行训练有了一定的思路。目前算法思路如下：
### 算法：好朋友sfl
服务器初始化W层模型的模型参数定义为$W_0$，客户端上传自身的状态信息（$D_i$，数据集大小;$C_i$:计算能力;$P_i$数据集数据分布，$(X_i,Y_I)$位置坐标)
服务器根据各客户端上传的状态信息，主要是数据分布之间的差异性以及计算能力,还有通信范围与位置远近，根据匈牙利算法算好匹配搭档。
匹配具体流程：
首先根据客户端其距离远近，计算能力，数据异质性的差异映射到权重矩阵里，然后利用匈牙利算法得到最佳匹配值。     
具体如下：位置差异：将客户端Ui位置分别定义成$(X_i,Y_i)$,客户端之间的间隔距离是 $disti=(X_i-X_j)^2＋（Y_i-Y_j)^2.$将通信范围定义100,有两种特殊情况，若间隔距离都大于100，或者算的自己与自己的位置距离，我们将这两种特殊情况下通信距离都定义成无穷。总的生成的位置差异矩阵设成$dist\_matrix$。

计算能力：不同客户端的计算能力定义成$C_i$，显然那计算能力的差异可以用最小二乘法表示成$(C_i-C_j)^2$，当是自己和自己的时候，这种情况不是我们想要考虑的，所以我们将其设为-1.总的计算能力矩阵我们将其定义成$C$

数据异质性：根据客户端定义几种不同类型的分布，此处定义成均匀，高斯，对数，指数分布的数据，然后我们利用迪利克雷分布将其范围缩至$(0,1)$之间，再利用Kl散度分别表示出不同分布之间的差异值，由于KL散度正常情况输出值也肯定是非负数，所以自己和自己计算时，我们将其KL散度设置成为-1.总的数据差异矩阵设为d.
数据集大小的差异：$dataset\_size\_difference$，也用最小二乘法表示。

位置差异，计算能力，数据异质性我们解决之后，我们将其映射到一个w权重矩阵里，定义成
$$ w = dist\_matrix / (1/2 * c + 1/4* d+1/4*dataset\_size\_difference)。$$

再根据匈牙利算法输入我们的成本矩阵W，然后得到最佳匹配的结果$（U_i，V_j)$。
同时服务器计算客户端的向前传播步长和总聚合权重（这里总聚合权重主要指配对后的两个客户端当作一个整体，每个整体之间的总数据集的大小显然不同，为了方便之后服务器聚合时加权进行，故需要定义。）组队后将其定义为$M$，每两个组队后的客户端将其定义成一个$M_i$,M中一共包含M个$M_i$，即组队后的两个小客户端的数据集总数为$D$，即每一个$D_i$里面是$d_i$的大小加上$d_j$的大小，由此算出组队后聚合权重是$Ai=\frac{D_i}{\sum_{r=1}^m D_i}$，服务器计算其传播步骤$L_i=\frac{C_i}{C_i+C_j}W$;(此处的i,j是匹配好的对应的结果）,内部聚合权重为 $a_i=\frac{d_i}{d_i+d_j}.     
其中一个小客户端相较于所有客户端聚合权重其实应是 A_j * a_i服务器发送（W_0,匹配好的搭档V_j,L_i,a_i)给每个客户端U_i$。
图片是目前设想的伪代码：
![%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-03-22%20195316.png](attachment:%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-03-22%20195316.png)
里面具体内部向前传播过程
在环内正常情况会产生2个客户端，他们的位置距离是很近的，这个因素在传播过程中就不需要再考虑了。
在每个通信轮次中，匹配后的两个客户端和两个客户端之间是没有通信的，但是匹配后客户端和服务器之间是并联的，每个客户端的前向传播可以分成2个部分；我们将这两个客户端分别设置为A，B。假设一个神经网络的层数一共有N+1层，即A拥有的神经网络层数Fa←{L0，L1，...Ln},B拥有的神经网络层数Fb←{Ln+1,Ln+2,...LN}。
### 6G大会相关内容
对6G的概念有了一定的认知，同时对6G的相关创新有所了解，例如基于现今无线网络的新通信方法必然产生，包括THZ和RISs,AI通信，以及其适应性和可持续性方面的考虑；然后计算创新方面：主要是基于边缘创新方面，关于AI创新主要是知识驱动，至于AI那种是我们需要的？有几个方面：解释性（即所有模型必须执行可解释性的决定和预测）；有根据性（即所有模型都有能力去分析，理解，和重现）概括性（即所有模型有泛化能力，和可转移的学习能力）分割和合作性（这个应该就是联邦学习方面的研究），绿色可持续性，以及终身和可连续性（从而确保学习不会断层）
### 下周计划：
尝试将匹配好的客户端的前向传播训练代码解决。


```python
import numpy as np
import math
from scipy.spatial import distance
from scipy.stats import entropy
from scipy.stats import dirichlet
#算计算能力的差异
# 随机生成四个一维点坐标
points = np.random.rand(4, 1) * 200#4个客户端计算能力随机为0-200
c = np.zeros((len(points),len(points)))
for i in range(len(points)):
    for j in range(len(points)):
        if i != j:
            c[i][j] = math.pow(points[i] - points[j],2)
        else:
            c[i][j]= -1#由于计算能力采用的是最小二乘法，所以合理情况下，不能为负数，为负数的情况指的是自己和自己，相当于矩阵的对角线
# print(c)
        
#计算距离的差异性
# 随机生成四个二维点坐标
points1 = np.random.rand(4, 2) * 200

# 计算不同点之间的距离矩阵
dist_matrix = distance.cdist(points1, points1, 'euclidean')
#设置通信范围
communication_range = 100
for i in range(len(dist_matrix)):
    for j in range(len(dist_matrix[i])):
        if dist_matrix[i][j] > communication_range  or dist_matrix[i][j] == 0:#当客户端自己和自己时，距离设成无穷，以及大于通信范围时。
            dist_matrix[i][j] = 99999999
# print(dist_matrix)

#考虑数据差异性
d = np.zeros((len(points),len(points))) 
# 生成均匀分布的数据
uniform_data =  np.random.dirichlet(np.random.uniform(0, 1, 10))

# 生成高斯分布的数据
gaussian_data = (np.random.normal(0, 1, 10))

# 生成对数分布的数据
log_data = np.random.dirichlet(np.random.lognormal(0, 1, 10))

# 生成指数分布的数据
exponential_data = np.random.dirichlet(np.random.exponential(1, 10))
 

# 对高斯分布和指数分布数据进行截断
truncated_gaussian_data =  np.random.dirichlet(np.clip(gaussian_data, 0.1, 1))#阶段
truncated_exponential_data = np.clip(exponential_data, 0.1, 1)

# 对对数分布数据进行对数变换，可以将其转化成正态分布的数据
log_transformed_data = np.log(log_data)
#再将转化成正态分布的数据进行一个截断。
truncated_log_transformed_data = np.clip(log_transformed_data, 0.1, 1)

temp = []
temp.append(uniform_data)
temp.append(truncated_gaussian_data)
temp.append(truncated_log_transformed_data)
temp.append(truncated_exponential_data)
for i in range(len(temp)):
    for j in range(len(temp)):
        if i != j:
            d[i][j] = entropy(temp[i],temp[j])
        else:
            d[i][j] = -1
# print(c)
#计算数据集大小的差异
dataset_size=np.random.rand(4, 1) * 10000
dataset_size_difference= np.zeros((len(points),len(points)))
for i in range(len(dataset_size)):
    for j in range(len(dataset_size)):
        if i != j:
            dataset_size_difference[i][j] = math.pow(dataset_size[i] - dataset_size[j],2)
        else:
            dataset_size_difference[i][j]= -1
w = dist_matrix / (1/2 * c + 1/4 * d + 1/4 *dataset_size_difference )
# print(dataset_size_difference)
for i in range(len(w)):
    for j in range(len(w[i])):
        if i == j:
            w[i][j] = 19999
        if w[i][j] == math.inf:
            w[i][j] = 10000
# print(w)
#输出权重矩阵       

from scipy.optimize import linear_sum_assignment as linear_assignment# 求解最小权重完美匹配问题
assignments = linear_assignment(w)
print(assignments)
#利用匈牙利算法成功匹配

 
dataset_size_number = np.zeros((len(dataset_size)))
# print(len(assignments[0]) // 2)
for i in range(len(assignments[0])):
     dataset_size_number[i] = math.floor(dataset_size[assignments[0][i]]) + math.floor(dataset_size[assignments[1][i]])
# print(dataset_size)
# print(dataset_size_number)
all_dataset_size=sum(dataset_size_number)/2
A=np.zeros(len(dataset_size_number))
for i in range(len(A)):
    A[i]=dataset_size_number[i]/all_dataset_size
    
L = np.zeros((len(dataset_size)))
weight = np.random.rand(1) * 10000

for i in range(len(dataset_size)):
    L[i] = (points[assignments[0][i]] / (points[assignments[0][i]] + points[assignments[1][i]])) * weight[0]
print(L)

bingzaiyiqi =np.zeros((4,2))
for i in range(len(assignments[0])):
    bingzaiyiqi[i][0]=assignments[0][i]
    bingzaiyiqi[i][1]=assignments[1][i]
print(bingzaiyiqi)
```

    (array([0, 1, 2, 3], dtype=int64), array([1, 0, 3, 2], dtype=int64))
    [4850.46858271 4450.41405403 5491.50746918 3809.37516756]
    [[0. 1.]
     [1. 0.]
     [2. 3.]
     [3. 2.]]
    
